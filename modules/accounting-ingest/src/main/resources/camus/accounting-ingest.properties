camus.job.name=accounting camus
etl.destination.path=/default/temp/topics
etl.execution.base.path=/default/temp/accounting/exec
etl.execution.history.path=/default/temp/accounting/camus/exec/history
camus.message.decoder.class.accounting=com.linkedin.camus.etl.kafka.coders.XmlSplitMessageDecoder
etl.partitioner.class.accounting=com.linkedin.camus.etl.kafka.partitioner.TopicGroupingPartitioner
mappred.map.tasks=5
kafka.max.pull.hrs=1
kafka.max.historical.days=3
kafka.max.pull.minutes.per.task=-1
kafka.blacklist.topics=
kafka.whitelist.topics=accounting
log4j.configuration=true
kafka.client.name=camus
kafka.brokers=localhost:9092


etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily

etl.ignore.schema.errors=false

mapred.output.compress=false
etl.output.codec=deflate
etl.deflate.level=6
#etl.output.codec=snappy

etl.default.timezone=America/Los_Angeles
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

# Configures a customer reporter which extends BaseReporter to send etl data
#etl.reporter.class

mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.StringRecordWriterProvider
